import asyncio
import re
from base64 import b64decode
from datetime import datetime, timedelta

import aiohttp
import dateutil.parser
import dateutil.tz
from bs4 import BeautifulSoup


async def remove_keyword(text, keywords):
    '''å°‡æ–‡ç« æœ« å„å®¶æ–°èçš„å»£å‘Šåˆªé™¤'''
    for keyword in keywords:
        if keyword in text:
            text = text.split(keyword)[0]
    return text


async def fetch(url, categories=None):
    '''æŠ“å–urlä¸Šçš„æ¨™é¡Œã€æ™‚é–“ã€ä½œè€…ã€å…§æ–‡èˆ‡å…¶ä»–ç…§ç‰‡'''
    print(url)
    output = {"Source_id": None, 
              "Title": None, 
              "Time": None, 
              "Categories": categories,
              "Author": None, 
              "Content": None,
              "Other_picture": []}
    
    # å–å¾—source_id
    url_split = str(url).split('/')
    output['Source_id'] = url_split[-1]

    async with aiohttp.ClientSession() as session:
        async with session.post(
            "https://api.zyte.com/v1/extract",
            auth=aiohttp.BasicAuth("5c532da8add642e6bf662951b506adac", ""),
            json={
                "url": url,
                "httpResponseBody": True,
            }
        ) as api_response:
            api_response_json = await api_response.json()
            http_response_body = b64decode(api_response_json["httpResponseBody"])
            soup = BeautifulSoup(http_response_body, 'html5lib')  # è§£ææˆHTMLæ¨¹ç‹€çµæ§‹
            
            if soup.find('figure', class_='entityVideoPlayer-wrapper'):
                print('video')
                return 0

            # æŠ“å–æ¨™é¡Œ
            title = soup.find('h1')
            title_strip = title.get_text().replace('\u3000', ' ').strip()
            output['Title'] = title_strip

            # æŠ“å–æ–‡ç« é ­çš„ç…§ç‰‡èˆ‡æ–‡å­—
            picture = soup.find('div', class_='image-wrapper image-wrapper-withsizes')
            if picture:
                img = picture.find('img').get('src')
            else:
                img = ''

            article = soup.find('article', class_='news-content textSize--md')
            # åˆªé™¤ä¸å¿…è¦å…§å®¹
            ul = article.find('ul')
            if ul:
                for element in ul.find_all_next():
                    element.decompose()  # åˆªé™¤è©²æ¨™ç±¤åŠå…¶æ‰€æœ‰å¾Œä»£
                ul.decompose()  # åˆªé™¤å»¶ä¼¸é–±è®€

            # æŠ“å–å…§å®¹
            keywords = ['å»¶ä¼¸é–±è®€', 'ï¼Šç·¨è€…æŒ‰ï¼š', 'ä¸‹è¼‰ã€Œè²¡è¨Šå¿«å ±Appã€æœ€å³æ™‚æœ€å°ˆæ¥­æœ€æ·±åº¦', 'ç«‹åˆ»åŠ å…¥',
                                'ã€Šæ°‘è¦–æ–°èç¶²ã€‹æé†’æ‚¨', 'æ›´å¤š', 'æƒ³å¿«é€ŸçŸ¥é“', '1. äº«å—æ›´é«˜è³ªé‡çš„è²¡ç¶“å…§å®¹ é»æˆ‘åŠ å…¥ç¶“æ¿Ÿæ—¥å ±å¥½å‹',
                                'åŠ å…¥ã€Šå·¥å•†æ™‚å ±ã€‹LINEå¥½å‹', '4. ã€ŠğŸ‘‰åŠ å…¥æ°‘è¦–æ–°èLineå¥½å‹ï¼Œé‡é»æ–°èä¸æ¼æ¥ğŸ‘ˆã€‹', 'åŠ å…¥ã€ˆè²¡ç¶“Må¹³æ–¹ã€‰å®˜æ–¹Line',
                                'æƒ³å¿«é€ŸçŸ¥é“', 'ç«‹å³åŠ å…¥ã€ŠTVBSå¨›æ¨‚é ­æ¢ã€‹']
            content = article.find_all_next(['p', 'h3'], href=False)
            text = [element.get_text() for element in content]
            text = ''.join(text)
            text = await remove_keyword(text, keywords)
            output['Content'] = str(img) + str(text)  # æ–‡ç« é ­çš„åœ–ç‰‡èˆ‡ç…§ç‰‡åŠ å…¥content
            
            # æŠ“å–æ™‚é–“
            div = soup.find('div', class_='entityPublishInfo-meta')
            time = div.find('span')
            time = str(time.get_text().strip()).split(' â€¢ ')

            pattern = r"ç™¼å¸ƒæ–¼"
            ago = ''
            for t in time:  # æŠ“å‡ºç™¼å¸ƒæ—¥æœŸ
                if re.search(pattern, t): 
                    ago = t

            current_time = datetime.now().replace(microsecond=0)
            date = ''
            if 'å°æ™‚' in ago:
                match = re.search(r'\d+', ago)
                number = int(match.group())
                new_time = current_time - timedelta(hours=number)
            elif 'åˆ†é˜' in ago:
                match = re.search(r'\d+', ago)
                number = int(match.group())
                new_time = current_time - timedelta(minutes=number)
            elif 'å¤©' in ago:
                match = re.search(r'\d+', ago)
                number = int(match.group())
                new_time = current_time - timedelta(days=number)
            elif 'å¹´' in ago:   
                new_time = datetime.strptime(f"{ago[3:].strip()}", "%Yå¹´%mæœˆ%dæ—¥%H:%M")
            else:
                new_time = datetime.strptime(f"{current_time.year}å¹´{ago[3:].strip()}", "%Yå¹´%mæœˆ%dæ—¥%H:%M")

            time_plus = str(new_time) + '+08:00'
            time_turn = dateutil.parser.parse(time_plus).astimezone(dateutil.tz.UTC)
            output['Time'] = date + str(time_turn)

            # æŠ“å–ä½œè€…
            aut = ''
            for t in time:
                if not any(char.isdigit() for char in t):
                    aut = t
            if aut:
                if '\u3000' in aut:
                    aut = aut.split('\u3000')[0]
                if 'ï¼' in aut:
                    aut = aut.split('ï¼')[0]

            author = div.find('a').text.strip()
            output['Author'] = author + aut

            # æŠ“å–å…§æ–‡ä¸­çš„ç…§ç‰‡
            img_tags = article.find_all_next('img')
            if img_tags:
                image_urls = [img.get('src') for img in img_tags[1:] if img.get('src') != '']
                output['Other_picture'] = image_urls

            print(output)
            return output


async def main():
    url = 'https://today.line.me/tw/v2/article/JPmG22K'
    await fetch(url)


if __name__ == '__main__':
    asyncio.run(main())